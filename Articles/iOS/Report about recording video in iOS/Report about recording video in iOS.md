Довольно часто в мобильных приложениях используется функция записи видео и его
обработка. И здесь компания Apple, как всегда, пошла на встречу разработчикам и
добавила простую но эффективную библиотеку AVFoundation для работы с медиа
файлами. С помощью небольшого количества кода можно легко записать видео, а
затем несколько видеофайлов объединить в один. Но если впервые посмотреть на
этот код, то он покажется сложным, с большим количеством классов и с большим
количеством непонятных манипуляций с этими классами. Но если разобраться с
принципом работы, то вскоре все становится на свои места, все становится простым
и понятным.

Для начала определимся с логически понятными вещами, такими как:

1.  В нашего устройства, будь то iPhone 5 или iPad mini, есть камера;

2.  Также есть микрофон;

3.  И конечно же экран, куда без него.

Для записи видео этого достаточно, если не учитывать процессор и операционную
систему. Нужно просто картинку с камеры отобразить на экране. В AVFoundation все
так и происходит. Класс AVCaptureDevice отвечает за работу камеры, а класс
AVCaptureVideoPreviewLayer отображает изображение на экране. Для того, что бы
гибко и качественно организовать их взаимодействие был создан класс
AVCaptureSession. Он описан наборами входов AVCaptureInput и наборами выходов
AVCaptureOutput.

И так, что бы начать трансляцию видео с камеры, нам для начала нужно добавить
девайс в сессию, как источник видеоинформации. Для этого используется
специализованный класс AVCaptureDeviceInput, который является наследником
AVCaptureInput. С его помощью мы привязываем камеру к сессии. Микрофон
добавляется таким же образом, просто создается второй AVCaptureDeviceInput и
второй AVCaptureDevice, с указанием того, что это не камера, а микрофон.

![](<report_1_image_1.png>)

Затем нужно создать AVCaptureVideoPreviewLayer на основе существующей сессии,
которая передается в конструкторе класса, и добавить его на UIView в нашем
UIViewController. Теперь мы готовы транслировать изображение с камеры на
представление в нашем приложении. И для этого нам нужно просто запустить сессию.

Когда у нас уже есть красивая картинка на экране iPhone, мы хотим сохранить ее в
видеофайл. Здесь тоже нет ничего сложного, просто нужно создать AVCaptureOutput,
который информацию с сессии будет складывать, буфер за буфером, в отдельный
файл. Для этого создаем объект специализированного класса
AVCaptureMovieFileOutput, который как уже догадались, является расширенным
наследником AVCaptureOutput. Указываем URL, путь в файловой системе, где будет
хранится, ново созданный файл. Главное не забыть начать процесс записи, потому
что просто добавить недостаточно.

![](<report_1_image_2.png>)

Сейчас у нас уже видео не только отображается на экране, но и записывается в
файл, который мы сможем позже пересмотреть. А если мы записали два и больше
видео и хотим объединить их в один фильм? Для этого не обязательно искать
специализированные сложные программы для обработки видео. Мы сами можем написать
такой алгоритм, используя AVFoundation. Что бы понять как мы это сделаем, начнем
с рассмотрения видеофайла.

В самом распространенном виде, видеофайл состоит из двух треков. Это видеотрек и
аудиотрек. Если у нас есть несколько видеофайлов, то мы естественно имеем
несколько видеотреков и несколько аудиотреков. А нам нужно получить один общий
видик, то есть один общий видеотрек и один аудиотрек. Для этого нам нужно все
исходные видео файлы разделить на треки, а эти треки соединить последовательно
друг за другом. Получившиеся в результате видео и аудио треки нужно объединить в
видеофайл.

Как это делается в AVFoundation? Какие классы используются?

Для этого используются классы AVAsset, для представления файла, и AVAssetTrack,
для представления трека. Самим процессом воссоздания видеофайла с треков
занимается класс AVAssetExportSession. При этом видеотреки обьединяются
отдельно, а аудиотреки отдельно в AVMutableCompositionTrack. Две композиции
треков обьединяются в общую композицию AVMutableComposition, которую использует
сессия. Указываем экспорт сессии путь в файловой системе для нового файла и
запускаем ее на обработку.

С логикой работы с видео разобрались и теперь можем рассмотреть примеры кода.

Работу с камерой вынесем в отдельный класс VRSCameraWrapper. Интерфейс класса:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
@interface VRSCameraWrapper : NSObject
@property (nonatomic, weak) UIView *previewView;
@property (nonatomic, assign) UIInterfaceOrientation videoOrientation;
- (instancetype)initWithPreviewView:(UIView *)aPreviewView;
- (void)setupCamera;
- (void)flipCamera;
- (void)addVideoWritter:(id<VRSVideoWritterProtocol>)aVideoWritter;
- (void)removeVideoWritter:(id<VRSVideoWritterProtocol>)aVideoWritter;
@end
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

В методе setupCamera выполняются все необходимые действия по настройке сессии и
камеры.

Для начала создаем сессию

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
self.captureSession = [[AVCaptureSession alloc] init];
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Создаем и добавляем камеру

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
AVCaptureDevice *videoDevice = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];
self.videoInputDevice = [AVCaptureDeviceInput deviceInputWithDevice:videoDevice error:&error];
if ([self.captureSession canAddInput:self.videoInputDevice]) {
    [self.captureSession addInput:self.videoInputDevice];
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Отображаем видео на экран

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
self.previewLayer = [[AVCaptureVideoPreviewLayer alloc] initWithSession:self.captureSession];
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

После всех приготовлений запускаем сессию

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[self.captureSession startRunning];
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Логика записи видеофайлов вынесена в отдельный класс VRSVideoWritter. Его
описание:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
@interface VRSVideoWritter : NSObject <VRSVideoWritterProtocol>
@property (nonatomic, readonly) BOOL recording;
@property (nonatomic, assign) double maxDuration;
@property (nonatomic, assign) UIInterfaceOrientation videoOrientation;
@end
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Для корректного взаимодействия VRSCameraWrapper и VRSVideoWritter был определен
общий протокол записи VRSVideoWritterProtokol:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
@protocol VRSVideoWritterProtocol <NSObject>
@required
- (void)addCaptureSession:(AVCaptureSession *)aCaptureSession;
- (void)removeCaptureSession:(AVCaptureSession *)aCaptureSession;
- (void)startRecording;
- (void)stopRecording;
@end
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

В методе addCaptureSession: создается и добавляется AVCaptureMovieFileOutput:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
self.movieFileOutput = [[AVCaptureMovieFileOutput alloc] init];
[self.captureSession beginConfiguration];
if ([self.captureSession canAddOutput:self.movieFileOutput]) {
    [self.captureSession addOutput:self.movieFileOutput];
}
self.videoOrientation = UIInterfaceOrientationLandscapeRight;
[self.captureSession commitConfiguration];
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

В методе startRecording запускаем процесс записи видео

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[self.movieFileOutput startRecordingToOutputFileURL:outputURL recordingDelegate:self];
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Хотя был представлен не полный код, а только ключевые моменты, но все же его не
так много. Он демонстрирует простоту работы с AVFoundation и в тоже время ее
гибкость и возможности. Код взят с нашего семпла VideoRecordSample, котрый можно
найти на GitHub по ссылке <https://github.com/iQueSoft/iOSDemo_VideoRecord> .
